<!DOCTYPE html>



  


<html class="theme-next muse use-motion" lang="">
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">



  
  
    
    
  <script src="/lib/pace/pace.min.js?v=1.0.2"></script>
  <link href="/lib/pace/pace-theme-minimal.min.css?v=1.0.2" rel="stylesheet">







<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">




  
  
  
  

  
    
    
  

  

  
    
      
    

    
  

  
    
      
    

    
  

  
    
      
    

    
  

  
    
    
    <link href="//fonts.cat.net/css?family=Roboto Slab:300,300italic,400,400italic,700,700italic|Roboto Slab:300,300italic,400,400italic,700,700italic|Lobster Two:300,300italic,400,400italic,700,700italic|PT Mono:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.3" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.3">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.3">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.3">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.3" color="#222">





  <meta name="keywords" content="Algorithm,">





  <link rel="alternate" href="/atom.xml" title="Beyond" type="application/atom+xml">






<meta name="description" content="这篇笔记用于记录自己对Deep Q-learning的理解、思考 主要参考了post from Intel">
<meta name="keywords" content="Algorithm">
<meta property="og:type" content="article">
<meta property="og:title" content="Deep Q-learning Algorithm">
<meta property="og:url" content="http://notes-hongbo.top/2019/03/04/Deep-Q-learning-Algorithm/index.html">
<meta property="og:site_name" content="Beyond">
<meta property="og:description" content="这篇笔记用于记录自己对Deep Q-learning的理解、思考 主要参考了post from Intel">
<meta property="og:locale" content="default">
<meta property="og:image" content="http://notes-hongbo.top/2019/03/04/Deep-Q-learning-Algorithm/DQN.png">
<meta property="og:image" content="http://notes-hongbo.top/2019/03/04/Deep-Q-learning-Algorithm/2.png">
<meta property="og:image" content="http://notes-hongbo.top/2019/03/04/Deep-Q-learning-Algorithm/3.png">
<meta property="og:updated_time" content="2019-03-09T13:13:58.542Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Deep Q-learning Algorithm">
<meta name="twitter:description" content="这篇笔记用于记录自己对Deep Q-learning的理解、思考 主要参考了post from Intel">
<meta name="twitter:image" content="http://notes-hongbo.top/2019/03/04/Deep-Q-learning-Algorithm/DQN.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '5.1.3',
    sidebar: {"position":"right","display":"hide","offset":12,"b2t":false,"scrollpercent":true,"onmobile":true},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: 'undefined',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://notes-hongbo.top/2019/03/04/Deep-Q-learning-Algorithm/">





  <title>Deep Q-learning Algorithm | Beyond</title>
  








</head>

<body itemscope="" itemtype="http://schema.org/WebPage" lang="default">

  
  
    
  

  <div class="container sidebar-position-right page-post-detail">
    <div class="headband"></div>

    <a href="https://github.com/qhb1001" class="github-corner" aria-label="View source on GitHub"><svg width="80" height="80" viewbox="0 0 250 250" style="fill:#151513; color:#fff; position: absolute; top: 0; border: 0; right: 0;" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"/><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"/><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"/></svg></a><style>.github-corner:hover .octo-arm{animation:octocat-wave 560ms ease-in-out}@keyframes octocat-wave{0%,100%{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}@media (max-width:500px){.github-corner:hover .octo-arm{animation:none}.github-corner .octo-arm{animation:octocat-wave 560ms ease-in-out}}</style>


    
    <header id="header" class="header" itemscope="" itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Beyond</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br>
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            Archives
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br>
            
            About
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://notes-hongbo.top/2019/03/04/Deep-Q-learning-Algorithm/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="秦baibai">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.JPG">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Beyond">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">Deep Q-learning Algorithm</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-03-04T19:18:10+08:00">
                2019-03-04
              </time>
            

            
              <span class="post-meta-divider">|</span>
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-check-o"></i>
              </span>
              
                <span class="post-meta-item-text">Post modified&#58;</span>
              
              <time title="Post modified" itemprop="dateModified" datetime="2019-03-09T21:13:58+08:00">
                2019-03-09
              </time>
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/Deep-Reinforcement-Learning/" itemprop="url" rel="index">
                    <span itemprop="name">Deep Reinforcement Learning</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Words count in article&#58;</span>
                
                <span title="Words count in article">
                  2.7k
                </span>
              

              

              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>这篇笔记用于记录自己对Deep Q-learning的理解、思考</p>
<p>主要参考了<a href="https://www.intel.ai/demystifying-deep-reinforcement-learning/#gs.posGScBm" target="_blank" rel="noopener">post from Intel</a></p>
<a id="more"></a>
<h1 id="Deep-Q-Learning-Network"><a href="#Deep-Q-Learning-Network" class="headerlink" title="Deep Q-Learning Network"></a>Deep Q-Learning Network</h1><p>要解决什么问题？为什么要两者结合？</p>
<p>可以将问题的解决看成一个黑盒，深度学习解决的是一端到黑盒，而强化学习负责的就是黑盒到一端</p>
<p>深度学习的一大用处就是特征提取</p>
<h2 id="1-我们为什么需要深度学习？"><a href="#1-我们为什么需要深度学习？" class="headerlink" title="1. 我们为什么需要深度学习？"></a>1. 我们为什么需要深度学习？</h2><p>对于Breakout游戏，如果单纯地使用RL，那么只有将灰度图设置为状态，因此状态的数量将会是巨大的，根本无法处理。而如果使用DL，就可以通过设置一个神经网络，输入灰度图，输出对应的奖励，因此能够很方便地实现RL中的reward function而不必担心维度多大。</p>
<p>需要注意的是，针对该游戏，仅仅使用一张灰度图是无法确定游戏所处的状态的。因为小球的弹射方向以及速度无法从一张图中得到，所以使用两个连续的灰度图，即可解决该问题。</p>
<p>由于我还没有实现过CNN，因此此处我决定产生一个中断，先去学习CNN。</p>
<h3 id="1-1-CNN-by-Andrew-Ng"><a href="#1-1-CNN-by-Andrew-Ng" class="headerlink" title="1.1 CNN by Andrew Ng"></a>1.1 CNN by Andrew Ng</h3><h4 id="1-1-1-Motivation"><a href="#1-1-1-Motivation" class="headerlink" title="1.1.1 Motivation"></a>1.1.1 Motivation</h4><p>很主要的就是为了应对输入的图像规模过大，如果使用传统的全连接网络，那么根本无法处理。</p>
<p>所谓卷积，有两点好处</p>
<ul>
<li><p><strong>parameters sharing</strong></p>
<p>大大减少网络中的参数</p>
</li>
<li><p><strong>sparsity of connections</strong></p>
<p>相当于输出中的一个值仅与输入中的局部值相关</p>
<p>充分利用图像中的局部特征，而非通过全连接的方式使得输出受到图像中全部因素的影响</p>
</li>
</ul>
<h4 id="1-1-2-Calculation"><a href="#1-1-2-Calculation" class="headerlink" title="1.1.2 Calculation"></a>1.1.2 Calculation</h4><p>一般而言，CNN的结构通常是三层结构，前两层是卷积层，最后一层是全连接层</p>
<p>而卷积层实际上包含两部分，卷积以及池化。这里记录一下对其中参数的思考：</p>
<ul>
<li><p>convolution</p>
<p>输入为 $H\times W \times C​$，表示共有C个通道，每个通道实际上就是一个灰度图矩阵，每个矩阵的规模为$H \times W​$</p>
<p>假设filter的结构为f(filter size), s(stride), p(padding)，filter的个数为N，那么输出的规模为</p>
<p>$[\frac{H - f + 2p}{s} +1] \times[ \frac{W - f + 2p}{s}+1]\times N$</p>
<p>需要注意的是，其中参数的个数为$(f^{[l]} <em> f^{[l]} </em> n^{[l-1]}_c + 1) * c_n^{[l]}$，并且，一般而言，都使用该四元组，用于表示filter</p>
</li>
<li><p>pooling</p>
<p>由于这只是一个提取图像中特征的方法，因此只含有初始化网络时规定的pooling矩阵的规模，这是一个<strong>超参数</strong>。</p>
<p>关于pooling，<a href="https://www.quora.com/Why-does-Geoff-Hinton-say-that-its-surprising-that-pooling-in-CNNs-work" target="_blank" rel="noopener">Hinton提出</a>，其意义在于突出局部的特征，而忽视其所处的具体位置。举个例子，当一只猫在图片的下面时，我们明确判断出这是猫；如果给出一张猫处于上方的图片，通过pooling，就可以抓住这个特征而忽视其所处的位置。</p>
<p>但是同时，这又是不合理的。我们不仅关心这个特征的存在与否，同时还关心该特征的具体位置。</p>
<blockquote>
<p>What Hinton rightly points out is that a pooling operation throws away a lot of information about where a feature occurs. A pool over a small grid will detect a feature regardless of where in that grid that feature occurs. The reason for this is to make the detection translation-invariant (see <a href="http://ufldl.stanford.edu/wiki/index.php/Pooling" target="_blank" rel="noopener">Pooling - Ufldl</a>)</p>
<p>For example, if you want to do face recognition, it may be important to know exactly where the position of the mouth is relative to say, the nose.  This can’t be done if the exact positional information is thrown away by repeated pooling operations. </p>
</blockquote>
</li>
</ul>
<h2 id="2-如何改进强化学习算法？"><a href="#2-如何改进强化学习算法？" class="headerlink" title="2. 如何改进强化学习算法？"></a>2. 如何改进强化学习算法？</h2><p>RL算法在使用一个非线性评估器作为Q值时，将会出现非常不稳定，甚至出现diverge的情况。这种情况出现的原因是：决策序列行为之间的关联性、Q值的小小改动可能会引起决策的巨大变动、数据分布的变动，进而导致实际Q值与估计Q值$\text{reward} + \gamma \max _\limits{a’}Q(s’, a’)​$差距过大。</p>
<p>为此，提出了两种机制来解决该矛盾：</p>
<ul>
<li><p>experience replay</p>
<p>通过将经验存储起来，随机采样以更新神经网络中的参数，以满足数据独立同分布的条件，避免决策之间的连带性和关联性。</p>
</li>
<li><p>iterative update</p>
<p>迭代更新策略，周期性更新Q值，以防止关联性的产生</p>
<p>实际上就是一个baseline</p>
</li>
</ul>
<p><img src="DQN.png" alt=""></p>
<h1 id="Deep-Deterministic-Policy-Gradient"><a href="#Deep-Deterministic-Policy-Gradient" class="headerlink" title="Deep Deterministic Policy Gradient"></a>Deep Deterministic Policy Gradient</h1><h2 id="1-Policy-Gradient"><a href="#1-Policy-Gradient" class="headerlink" title="1. Policy Gradient"></a>1. Policy Gradient</h2><p>所有包含有通过对策略$\pi$设定参数$\theta$，从experience中直接学习策略的方法，统称为策略梯度方法，因此该类方法可以说是基于策略的。而同时学习策略以及值函数的方法，称为actor-critic方法，其中action表示学到的策略，critic表示学到的值函数。</p>
<p>PG相对基于值函数的优点是，该策略最终将会逼近一个确定策略，而不像$\epsilon-greedy$那样仍然会有$\epsilon$的可能随机选择。尽管后者可以通过在softmax中添加一个temperature，以保证随着时间的推进最终可以得到一个确定性策略，但是temperature衰减的过程很难把握，并且初值也很难选择。</p>
<p>基于策略的方法的一个最显而易见的好处，就是可以在初始化阶段，在策略中加入自己的知识(偏好)，以得到一个很好的起始状态。</p>
<p>关于PG的证明可以看<a href="https://medium.com/@jonathan_hui/rl-policy-gradients-explained-9b13b688b146" target="_blank" rel="noopener">这里</a>，其中的解释非常棒，并且给出了推导出来的式子的一个直观解释。</p>
<p><img src="2.png" alt=""></p>
<p>观察该式，可以发现下面划线的项其实是一个极大似然估计，表示在给定策略下得到这条状态转移路线的可能性，之后乘以了reward值，以对策略进行更新。直观来看，可以十分确定的是$\nabla_\theta J(\theta) \sim  \Epsilon (r(s, a))$，但是具体应该按照什么样的比例更新多少是未知的。而极大似然估计正是一个十分合适的比例。</p>
<blockquote>
<p>In Deep Learning, a long sequence of multiplication with factors that are strongly correlated can trigger vanishing or exploding gradient easily. However, the policy gradient only sums up the gradient which breaks the curse of multiplying a long sequence of numbers.</p>
</blockquote>
<p>高方差问题：对于某个问题，可能存在多个完美的policy，使得期望的奖励值达到最大。再加上这是非确定性策略问题，因此对于目前已学到的参数，可能它正在接近某个最优解，但是当探索出一条到达另外某个最优解的情况时，将会偏离当前的方向向另外一个最优解靠近，因此导致难以收敛，也就是高方差问题。</p>
<p>通过两个trick来解决高方差问题</p>
<ul>
<li><p>Reward to go</p>
<p>通过观察梯度更新的式子，可以发现t-1时刻的奖励竟然会与未来的t时刻的策略梯度相乘。这是不符合马克多夫性质的。因此可以将原式修改为<br>$$<br>\nabla_\theta J(\theta) =\cfrac{1}{N}\sum_{i=1}^N [ \sum_{t=1}^T [ \nabla_\theta\log  \pi_\theta(a_{i,t}|s_{i,t}) \sum_{t’=t}^T r(s_{i,t’}, a_{i,t’}) ]]<br>$$</p>
</li>
<li><p>Baseline</p>
<p>对于后面的激励，可以使其减去一个常数，以达到减少方差并且不改变无偏性</p>
<p>关于该常数的选取，在教材中有指明，可以是任意的函数、变量，只要不是a的函数即可。一般而言，对于MDP，baseline应该是关于状态的函数。下面这段话对baseline如何加快收敛的过程，作了一个直观的解释：</p>
<blockquote>
<p>In some states all actions have high values and we need a high baseline to diﬀerentiate the higher valued actions from the less highly valued ones; in other states all actions will have low values and a low baseline is appropriate.</p>
</blockquote>
<p>也正因如此，一般选择</p>
<ul>
<li><p>当前episode的平均reward</p>
<p>$b = \frac{1}{N}\sum_{i=1}^N r(\tau)$</p>
</li>
<li><p>使用一个神经网络来拟合该值</p>
<p>将原式中的轨迹上奖励总和替换为$A^{\pi}(s_t, a_t) = Q^{\pi}(s_t, a_t) - V^{\pi}(s_t)$</p>
<p>这个变形感觉很费解，是否会影响无偏性呢？</p>
</li>
</ul>
</li>
</ul>
<p>上述描述的算法，称之为REINFORCE-with-baseline算法，对于policy以及state-value function分别使用一个神经网络进行学习。但是它仍然不属于actor-critic算法，因为其中的baseline并没有涉及利用之后的行为状态更新当前的参数，而仅仅是加快收敛速度的作用，所以这并不能称之为批评家。</p>
<p>正如我们在TD方法中看到的，当批评家对参数进行单步更新的时候，这些偏差以及对状态的依赖都是有益的。</p>
<h2 id="2-“CONTINUOUS-CONTROL-WITH-DEEP-REINFORCEMENT-LEARNING”"><a href="#2-“CONTINUOUS-CONTROL-WITH-DEEP-REINFORCEMENT-LEARNING”" class="headerlink" title="2. “CONTINUOUS CONTROL WITH DEEP REINFORCEMENT LEARNING”"></a>2. “CONTINUOUS CONTROL WITH DEEP REINFORCEMENT LEARNING”</h2><p>拜读DDPG论文</p>
<p>其中给出的论文提出背景感觉非常实在有用，尤其是关于policy gradient算法发明出来的路线，1997-2011-2014，如今想来真是令人振奋。</p>
<p>为了解决使用非线性评估器近似Q值不稳定的问题，这里使用了DQN中的两个trick</p>
<ul>
<li><p>replay buffer</p>
</li>
<li><p>soft update</p>
<p>相对于DQN中在某个给定的步长范围之内更新，这里是类似于多臂赌博机中的soft update，也就是给出一个参数，只更新部分</p>
</li>
</ul>
<p>为了提高模型的泛化能力，使得同一种网络架构能够处理多种不同的情景(不同的游戏可坑存在不同的状态，并且每个状态下的取值范围也可能有很大的波动性)，这里对数据进行了正则化。</p>
<p>正如模型中的第二个单词deterministic的含义，DDPG学得的是确定性策略，也就是说给定网络中的参数以及当前的状态，可以唯一确定出下一个状态。因此必须同经典的RL算法一样，平衡exploring &amp; exploiting，论文中的做法是对确定性策略的输出加上一个噪声，以进行exploiting。</p>
<p>下面对算法流程进行分析</p>
<p><img src="3.png" alt=""></p>
<p>大体上是一个actor-critic框架，每个角色都由两个神经网络构成，一个是evaluation网络，一个是target网络，用于减少关联性，加快收敛速度。</p>
<p>对于更新式J，这里记录一下对其的理解</p>
<ul>
<li>首先看求和号后面的第二项，这是对策略求梯度，表示当前行为的变化，也就是actor这一步相对于上一步的梯度，actor只是作出这一步，而且不知道当前策略的变化将会带来的影响</li>
<li>现在看求和号后面的第一项， 对当前的state-action function求梯度，以得知actor作出的改变究竟是好是坏</li>
</ul>
<p><strong>关于更新式的证明留待将来研究</strong></p>

      
    </div>
    
    
    

    

    

    
      <div>
        <ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>Post author:</strong>
    秦baibai
  </li>
  <li class="post-copyright-link">
    <strong>Post link:</strong>
    <a href="http://notes-hongbo.top/2019/03/04/Deep-Q-learning-Algorithm/" title="Deep Q-learning Algorithm">http://notes-hongbo.top/2019/03/04/Deep-Q-learning-Algorithm/</a>
  </li>
  <li class="post-copyright-license">
    <strong>Copyright Notice: </strong>
    All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/3.0/" rel="external nofollow" target="_blank">CC BY-NC-SA 3.0</a> unless stating additionally.
  </li>
</ul>

      </div>
    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/Algorithm/" rel="tag"># Algorithm</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2019/02/02/My-first-RL-related-paper/" rel="next" title="My first RL-related paper">
                <i class="fa fa-chevron-left"></i> My first RL-related paper
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2019/03/08/Graph-Neural-Networks-A-Review-of-Methods-and-Applications/" rel="prev" title="Graph Neural Networks: A Review of Methods and Applications">
                Graph Neural Networks: A Review of Methods and Applications <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
      <div id="sidebar-dimmer"></div>
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope="" itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image" src="/images/avatar.JPG" alt="秦baibai">
            
              <p class="site-author-name" itemprop="name">秦baibai</p>
              <p class="site-description motion-element" itemprop="description">So we beat on, boats against the current, borne back ceaselessly into the past.</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives">
              
                  <span class="site-state-item-count">20</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">4</span>
                  <span class="site-state-item-name">categories</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">19</span>
                  <span class="site-state-item-name">tags</span>
                </a>
              </div>
            

          </nav>

          
            <div class="feed-link motion-element">
              <a href="/atom.xml" rel="alternate">
                <i class="fa fa-rss"></i>
                RSS
              </a>
            </div>
          

          <div class="links-of-author motion-element">
            
              
                <span class="links-of-author-item">
                  <a href="https://github.com/qhb1001" target="_blank" title="GitHub">
                    
                      <i class="fa fa-fw fa-github"></i>GitHub</a>
                </span>
              
                <span class="links-of-author-item">
                  <a href="mailto:hongbo.qin.1001@gmail.com" target="_blank" title="E-Mail">
                    
                      <i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                </span>
              
                <span class="links-of-author-item">
                  <a href="https://www.instagram.com/qhb_1001/" target="_blank" title="Instagram">
                    
                      <i class="fa fa-fw fa-instagram"></i>Instagram</a>
                </span>
              
            
          </div>

          
          

          
          
            <div class="links-of-blogroll motion-element links-of-blogroll-inline">
              <div class="links-of-blogroll-title">
                <i class="fa  fa-fw fa-link"></i>
                友情链接
              </div>
              <ul class="links-of-blogroll-list">
                
                  <li class="links-of-blogroll-item">
                    <a href="http://mrsempress.top" title="Little Sun" target="_blank">Little Sun</a>
                  </li>
                
                  <li class="links-of-blogroll-item">
                    <a href="https://undefinedf.github.io/" title="UdefinedF" target="_blank">UdefinedF</a>
                  </li>
                
                  <li class="links-of-blogroll-item">
                    <a href="blog.three7.cc" title="37as" target="_blank">37as</a>
                  </li>
                
              </ul>
            </div>
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Deep-Q-Learning-Network"><span class="nav-number">1.</span> <span class="nav-text">Deep Q-Learning Network</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-我们为什么需要深度学习？"><span class="nav-number">1.1.</span> <span class="nav-text">1. 我们为什么需要深度学习？</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-1-CNN-by-Andrew-Ng"><span class="nav-number">1.1.1.</span> <span class="nav-text">1.1 CNN by Andrew Ng</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-1-1-Motivation"><span class="nav-number">1.1.1.1.</span> <span class="nav-text">1.1.1 Motivation</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-1-2-Calculation"><span class="nav-number">1.1.1.2.</span> <span class="nav-text">1.1.2 Calculation</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-如何改进强化学习算法？"><span class="nav-number">1.2.</span> <span class="nav-text">2. 如何改进强化学习算法？</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Deep-Deterministic-Policy-Gradient"><span class="nav-number">2.</span> <span class="nav-text">Deep Deterministic Policy Gradient</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-Policy-Gradient"><span class="nav-number">2.1.</span> <span class="nav-text">1. Policy Gradient</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-“CONTINUOUS-CONTROL-WITH-DEEP-REINFORCEMENT-LEARNING”"><span class="nav-number">2.2.</span> <span class="nav-text">2. “CONTINUOUS CONTROL WITH DEEP REINFORCEMENT LEARNING”</span></a></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">秦baibai</span>

  
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-area-chart"></i>
    </span>
    
      <span class="post-meta-item-text">Site words total count&#58;</span>
    
    <span title="Site words total count">48.4k</span>
  
</div>






  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Muse</a></div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
          <span id="scrollpercent"><span>0</span>%</span>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.3"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.3"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.3"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.3"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.3"></script>




  


  




	





  





  












  





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  
  <script type="text/javascript" src="/js/src/js.cookie.js?v=5.1.3"></script>
  <script type="text/javascript" src="/js/src/scroll-cookie.js?v=5.1.3"></script>


  


</body>
</html>
