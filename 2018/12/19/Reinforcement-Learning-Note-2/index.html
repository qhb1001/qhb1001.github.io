<!DOCTYPE html>



  


<html class="theme-next muse use-motion" lang="">
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">



  
  
    
    
  <script src="/lib/pace/pace.min.js?v=1.0.2"></script>
  <link href="/lib/pace/pace-theme-minimal.min.css?v=1.0.2" rel="stylesheet">







<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">




  
  
  
  

  
    
    
  

  

  
    
      
    

    
  

  
    
      
    

    
  

  
    
      
    

    
  

  
    
    
    <link href="https://fonts.googleapis.com/css/css?family=Roboto Slab:300,300italic,400,400italic,700,700italic|Roboto Slab:300,300italic,400,400italic,700,700italic|Lobster Two:300,300italic,400,400italic,700,700italic|PT Mono:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.3" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.3">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.3">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.3">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.3" color="#222">





  <meta name="keywords" content="Reinforcement Learning,">





  <link rel="alternate" href="/atom.xml" title="Beyond" type="application/atom+xml">






<meta name="description" content="Reinforcement Learning: An Introduction 学习笔记（2）">
<meta name="keywords" content="Reinforcement Learning">
<meta property="og:type" content="article">
<meta property="og:title" content="Reinforcement Learning Note (2)">
<meta property="og:url" content="http://notes-hongbo.top/2018/12/19/Reinforcement-Learning-Note-2/index.html">
<meta property="og:site_name" content="Beyond">
<meta property="og:description" content="Reinforcement Learning: An Introduction 学习笔记（2）">
<meta property="og:locale" content="default">
<meta property="og:image" content="http://notes-hongbo.top/2018/12/19/Reinforcement-Learning-Note-2/4.1.png">
<meta property="og:image" content="http://notes-hongbo.top/2018/12/19/Reinforcement-Learning-Note-2/4.2.png">
<meta property="og:image" content="http://notes-hongbo.top/2018/12/19/Reinforcement-Learning-Note-2/4.4.png">
<meta property="og:image" content="http://notes-hongbo.top/2018/12/19/Reinforcement-Learning-Note-2/4.3.png">
<meta property="og:image" content="http://notes-hongbo.top/2018/12/19/Reinforcement-Learning-Note-2/5.1.png">
<meta property="og:image" content="http://notes-hongbo.top/2018/12/19/Reinforcement-Learning-Note-2/5.2.png">
<meta property="og:image" content="http://notes-hongbo.top/2018/12/19/Reinforcement-Learning-Note-2/5.3.png">
<meta property="og:image" content="http://notes-hongbo.top/2018/12/19/Reinforcement-Learning-Note-2/5.4.png">
<meta property="og:image" content="http://notes-hongbo.top/2018/12/19/Reinforcement-Learning-Note-2/5.5.png">
<meta property="og:updated_time" content="2018-12-19T14:21:04.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Reinforcement Learning Note (2)">
<meta name="twitter:description" content="Reinforcement Learning: An Introduction 学习笔记（2）">
<meta name="twitter:image" content="http://notes-hongbo.top/2018/12/19/Reinforcement-Learning-Note-2/4.1.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '5.1.3',
    sidebar: {"position":"right","display":"always","offset":12,"b2t":false,"scrollpercent":true,"onmobile":true},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: 'undefined',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://notes-hongbo.top/2018/12/19/Reinforcement-Learning-Note-2/">





  <title>Reinforcement Learning Note (2) | Beyond</title>
  








</head>

<body itemscope="" itemtype="http://schema.org/WebPage" lang="default">

  
  
    
  

  <div class="container sidebar-position-right page-post-detail">
    <div class="headband"></div>

    <a href="https://github.com/qhb1001" class="github-corner" aria-label="View source on GitHub"><svg width="80" height="80" viewbox="0 0 250 250" style="fill:#151513; color:#fff; position: absolute; top: 0; border: 0; right: 0;" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"/><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"/><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"/></svg></a><style>.github-corner:hover .octo-arm{animation:octocat-wave 560ms ease-in-out}@keyframes octocat-wave{0%,100%{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}@media (max-width:500px){.github-corner:hover .octo-arm{animation:none}.github-corner .octo-arm{animation:octocat-wave 560ms ease-in-out}}</style>


    
    <header id="header" class="header" itemscope="" itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Beyond</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br>
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            Archives
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br>
            
            About
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://notes-hongbo.top/2018/12/19/Reinforcement-Learning-Note-2/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="秦baibai">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.JPG">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Beyond">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">Reinforcement Learning Note (2)</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-12-19T22:18:32-08:00">
                2018-12-19
              </time>
            

            
              <span class="post-meta-divider">|</span>
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-check-o"></i>
              </span>
              
                <span class="post-meta-item-text">Post modified&#58;</span>
              
              <time title="Post modified" itemprop="dateModified" datetime="2018-12-19T06:21:04-08:00">
                2018-12-19
              </time>
            
          </span>

          

          
            
          

          
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Words count in article&#58;</span>
                
                <span title="Words count in article">
                  2.9k
                </span>
              

              

              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p><em>Reinforcement Learning: An Introduction</em> 学习笔记（2）</p>
<a id="more"></a>
<p>[TOC]</p>
<h1 id="Chapter-4-Dynamic-Programming"><a href="#Chapter-4-Dynamic-Programming" class="headerlink" title="Chapter 4 Dynamic Programming"></a>Chapter 4 Dynamic Programming</h1><h2 id="Policy-Evaluation"><a href="#Policy-Evaluation" class="headerlink" title="Policy Evaluation"></a>Policy Evaluation</h2><p>基于策略的迭代算法，实际上就是穷举遍历的过程。在P93的图中，我可以预见，在第十六步之后，整个图的值函数将不会再发生巨大的变化，而是趋于一个稳定的状态。</p>
<p>这里需要注意的是，我们计算的对象，是一个确定策略的情景。也只有在确定策略的情景下，我们才可以明确出value function。</p>
<p><img src="4.1.png" alt=""></p>
<h2 id="Policy-Improvement"><a href="#Policy-Improvement" class="headerlink" title="Policy Improvement"></a>Policy Improvement</h2><p>$$<br>v_{\pi’}(s) \ge v_{\pi}(s)<br>$$</p>
<p>上式描述了，当新策略的每一步要么与原策略的收益相同，要么优于原策略，那么称当前的策略更优。于是我们可以使用贪心策略来启发式地从一个基础策略出发，对原有策略不断进行更新。<br>$$<br>\begin{equation}<br>\begin{aligned}<br>\pi’(s) &amp;=  \mathop{\arg\max}<em>{a}q</em>\pi(s,a) \<br>&amp;= \mathop{\arg\max}<em>a\mathbb{E}\big[R</em>{t+1} + \gamma v_\pi(S_{t+1})\ | \ S_t=s, A_t=a \big]<br>\end{aligned}<br>\end{equation}<br>$$<br>如此一来可以保证，当前策略一定优于原有策略。只是，针对某些问题，局部最优解不一定引向全局最优解。</p>
<h2 id="Policy-Iteration"><a href="#Policy-Iteration" class="headerlink" title="Policy Iteration"></a>Policy Iteration</h2><p>基于以上两点讨论，我们可以得到一个改进策略的基本方法<br>$$<br>\pi_0 \overset{\text{E}}{\rightarrow} v_{\pi_0} \overset{\text{I}}{\rightarrow} \pi_1 \overset{\text{E}}{\rightarrow} v_{\pi_1} \overset{\text{I}}{\rightarrow} \pi_2 \overset{\text{E}}{\rightarrow} \dots \overset{\text{I}}{\rightarrow} \pi_<em> \overset{\text{E}}{\rightarrow} v_{</em>}<br>$$<br>这里E表示evaluation，I表示Improvement。通过贪心策略我们可以保证，新生成的策略一定比原有策略更优。</p>
<p><img src="4.2.png" alt=""></p>
<p>由于我们使用的MDP是有限状态的，这也就表示状态的个数是有限的，因此在有限迭代之后一定可以找到一个最优值。不过该最优值可能是局部最优解。更加形式化的证明可以参见<a href="https://stats.stackexchange.com/questions/97083/why-does-policy-iteration-algorithm-converge-to-optimal-value-reinforcement-le" target="_blank" rel="noopener">这里</a>。</p>
<h2 id="Value-Iteration"><a href="#Value-Iteration" class="headerlink" title="Value Iteration"></a>Value Iteration</h2><p><img src="4.4.png" alt=""></p>
<p>上图展示了value iteration的工作方式。类比于上面论述的方法，这里的特点是，只有一次policy improvement，因此收敛得更快。</p>
<p><img src="4.3.png" alt=""></p>
<p>上图展示了两者的区别。很明显，value iteration丢失了某些信息，以达到更快的迭代速度。</p>
<p>类比到梯度下降法中的话，可以认为policy是batch gradient descent，而后者这是stochastic gradient descent。</p>
<p><a href="https://medium.com/@jaems33/gamblers-problem-b4e91040e58a" target="_blank" rel="noopener">这篇博客</a>很好地介绍了value iteration的工作方式，并且对gamble’s problem的最终结果进行了思考，得到了一个较好的解释：教材中的结果是不可复现的，具体的策略依赖于最终目标的数学特征。如1024由于是一个2的幂次，所以将会出现在2的幂次上面赌徒应该投入更多的筹码。</p>
<h1 id="Chapter-5-Monte-Carlo-Methods"><a href="#Chapter-5-Monte-Carlo-Methods" class="headerlink" title="Chapter 5 Monte Carlo Methods"></a>Chapter 5 Monte Carlo Methods</h1><p>蒙特卡洛方法，是一种可以通过学习来不断估计值函数，并且发现最优策略的方法。不像DP方法需要知道环境中的全部信息，蒙特卡洛方法只需要一些经验(experience) — 也就是通过与环境进行交互得到的必要信息。从这些信息中进行学习以达到最优行为。尽管蒙特卡洛方法也是需要建模的，但是这个模型只需要简单的转移即可，并不需要像DP那样的每一个状态的概率分布都必须建立。</p>
<p>这里只讨论处理episodic tasks的蒙特卡洛方法。</p>
<p>MC被广泛地应用于含有随机的情景下。MC与多臂赌博机问题有很多相似之处 – 都是通过采样、取均值的方法对state-action二元组进行估计；主要的不同是这里存在着多种状态，每一个状态都单独是一个多臂赌博机问题，并且各个状态之间是相互关联的。也就是说，在同一阶段，当前状态采取的行为将会影响下一个状态采取的行为获得的收益。但是相对于DP方法，各个状态之间又是相互独立的，原因是在DP中估计值函数需要一个状态之后的所有状态，而蒙特卡洛方法只需要一个状态而已，也就是采样到的那一个收益。</p>
<p>对于值函数与策略的迭代，MC同样使用了GPI(Generalized Policy Iteration)。</p>
<p>相对于DP，MC方法的优点总结如下：</p>
<ul>
<li>能够直接从经验中学习，而不需要建立理论模型</li>
<li>能够避免$p(s’, r|s, a)$的计算</li>
<li>能够只关注重要的状态而不去关心其它次要状态</li>
</ul>
<h2 id="Monte-Carlo-Prediction"><a href="#Monte-Carlo-Prediction" class="headerlink" title="Monte Carlo Prediction"></a>Monte Carlo Prediction</h2><p>在多臂赌博机问题中，预测state-action函数时，最简单的方法就是累积求和取平均，如此一来在试验次数足够大的情况下就可以收敛到真值。这是MC方法的核心思维。</p>
<p>在模型已知的情况下，通过收集每一个episode中经过某个状态的信息，我们就可以估计出该状态的值函数。估计方法有两种，一种称为<em>first-visit MC method</em>，表示只使用每一个episode中第一次访问该状态得到的信息；另一种方法称为<em>every-visit MC method</em>，表示使用每一次访问该状态得到的信息。这里只讨论第一种方法。</p>
<p><img src="5.1.png" alt=""></p>
<p>这个算法流程中有两点需要注意的地方：</p>
<ul>
<li>对于收益的计算是逆序进行的，其实是一种类似前缀和的思想。我们关心的所谓从一个状态出发的收益，正是<code>pre[n] - pre[i]</code>，这就是逆序求和的原因所在。</li>
<li><code>unless</code>在这里判断的，是要看该状态在本阶段是否已经计算过。因此本算法是<em>first-visit MC method</em>。</li>
</ul>
<h2 id="MC-Estimating-for-Action-Values"><a href="#MC-Estimating-for-Action-Values" class="headerlink" title="MC Estimating for Action Values"></a>MC Estimating for Action Values</h2><p> 当不存在模型的时候，就没有办法定义状态带来的收益了。因此想要估计的函数发生了改变，此时我们关心的是行为收益；当每一个状态后续的各种行为收益被估算出来之后，就可以很容易地进行策略更新。</p>
<p>此时我们估计的值就变成了state-action。同样的，估计方式有两个，分别是<em>first-visit MC method</em>以及<em>every-visit MC method</em>。含义与上述的基本一致。</p>
<p>这样的算法可能出现一些问题：</p>
<ul>
<li>在确定性策略的情况下，有些状态行为也许永远也访问不到。</li>
<li>我们使用的数据不可能包含有无限组数据，因此最终的结果一定是渐进收敛到真值，但不是真值。</li>
</ul>
<p>第一个问题实际上就是多臂赌博机问题中遇到的EE问题；而第二个问题在DP中也遇到了，这个问题存在于所有算法之中。</p>
<h3 id="MC-Exploring-Start"><a href="#MC-Exploring-Start" class="headerlink" title="MC Exploring Start"></a>MC Exploring Start</h3><p>初始化时，随机一点，允许探索行为。</p>
<p><img src="5.2.png" alt=""></p>
<p>缺点是不切实际，相当麻烦。</p>
<h3 id="On-policy-via-epsilon-soft"><a href="#On-policy-via-epsilon-soft" class="headerlink" title="On-policy via $\epsilon$-soft"></a>On-policy via $\epsilon$-soft</h3><p>MC方法中有两类方法可以良好地解决第一点问题，分别是on-policy, off-policy。这里首先介绍前者。</p>
<p>On-policy方法，一般来说都具有soft的特性，也就是$\pi(a|s)&gt;0$。但是在策略优化步骤中将会逐渐得演变为确定性策略。下面给出算法实现：</p>
<p><img src="5.3.png" alt=""></p>
<p>对于非贪心策略的可能性赋值那里很自然：每一个都有平等的机会被选中。但是对于贪心策略下的行为，这里将其被选中的策略规定为$1 - \epsilon + \epsilon/|A(S_t)|$。这是把$\epsilon$-soft思想放入算法内部的实现方式，具体的原因见下面的推导：<br>$$<br>\begin{equation}<br>\begin{aligned}<br>q_pi(s, \pi’(s)) &amp;= \sum_a \pi’(a|s)q_\pi(s,a) \<br>&amp;=\frac{\epsilon}{|A(s)|}\sum_aq_\pi(s,a) + (1-\epsilon)\max_aq_\pi(s,a) \<br>&amp;\ge \frac{\epsilon}{|A(s)|}\sum_aq_\pi(s,a)+(1-\epsilon)\sum_a\frac{\pi(a|s) - \frac{\epsilon}{|A(s)|}}{1-\epsilon}q_\pi(s,a) \<br>&amp;= \frac{\epsilon}{|A(s)|}\sum_aq_\pi(s,a)-\frac{\epsilon}{|A(s)|}\sum_aq_\pi(s,a) + \sum_a\pi(a|s)q_\pi(s,a) \<br>&amp;= v_\pi(s)<br>\end{aligned}<br>\end{equation}<br>$$<br>当然还有一种很自然的实现方式，那就是将探索的分类放到$\pi$函数外面。首先选取一个随机数，与$\epsilon$进行比较，大于该随机数时选择贪心策略，否则从非贪心策略中随机选择一个行为。这两种实现方法是等价的。</p>
<h3 id="Oﬀ-policy-Prediction-via-Importance-Sampling"><a href="#Oﬀ-policy-Prediction-via-Importance-Sampling" class="headerlink" title="Oﬀ-policy Prediction via Importance Sampling"></a>Oﬀ-policy Prediction via Importance Sampling</h3><p>在上面讨论的算法中，可以发现这里面存在着一个矛盾：一方面想要根据值函数来确定出策略，一方面想要进行探索以发现更好的值函数。因此On-Policy最终的策略包含了一定的探索成分，这使得其不是最优策略。一个显而易见的解决方法是维护两个策略，一个表示最终的目标策略(target policy)，一个用于产生行为的行为策略(behavior policy)。在这里，发生了数据与最终策略的分离，因此称为<em>off-policy learning</em>。</p>
<p>具体的做法是采用所谓的<em>importance-sampling</em>，也就是利用从另一个分布采样得到的值，来估计这一个分布中的期望值。给定起始状态$S_t$，以及后续状态直到$S_T$，在策略$\pi$下产生这一串决策的概率是：<br>$$<br>\begin{equation}<br>\begin{aligned}<br>\text{Pr}{&amp;A_t, S_{t+1},,A_{t+1},\dots,S_T|S_t, A_{t:T-1~\sim \pi} }\<br>&amp;=\pi(A_t|S_t)p(S_{t+1}|S_t, A_t)\pi(A_{t+1}|S_{t+1})\cdots p(S_T|S_{T-1},A_{T-1}) \<br>&amp;= \prod_{k=t}^{T-1}\pi(A_k|S_k)p(S_{k+1}|S_k,A_k)<br>\end{aligned}<br>\end{equation}<br>$$<br>在上式的基础上，产生<em>importance-sampling ratio</em>：<br>$$<br>\rho_{t:T-1} = \frac{\prod_{k=t}^{T-1}\pi(A_k|S_k)p(S_{k+1}|S_k,A_k)}{\prod_{k=t}^{T-1}b(A_k|S_k)p(S_{k+1}|S_k,A_k)} = \prod_{k=t}^{T-1}\frac{\pi(A_k|S_k)}{b(A_k|S_k)}<br>$$<br>可以观察到这是一个与模型无关，只与策略相关的方法。此时我们就可以通过策略$b$的观测值来得到策略$\pi$的期望值：<br>$$<br>\mathbb{E}[\rho_{t:T-1}G_t|S_t] = v_\pi(S_t)<br>$$<br>在具体实现的时候，我们有两种方式可以选择：</p>
<ul>
<li><p>ordinary importance sampling</p>
<p>这种方式对目标策略的估计公式为$V(s) = \displaystyle \frac{\sum_{t\in\mathcal{J}(s)}\rho_{t:T(t)-1}G_t}{|\mathcal{J}(s)|}$</p>
<p>表示对于每一次采样结果都平等对待。考虑当只有一次采样结果并且与实际值偏差较大的情况，那么可能会出现偏差非常大的结果。因此这是high variance的算法。</p>
</li>
<li><p>weighted importance sampling</p>
<p>这种方式对目标策略的估计公式为$V(s) = \displaystyle \frac{\sum_{t\in\mathcal{J}(s)}\rho_{t:T(t)-1}G_t}{\sum_{t\in\mathcal{J}(s)}\rho_{t:T(t)-1}} $</p>
<p>如此一来，每一个采样结果将会根据importance-sampling ratio的值进行区别对待。因此不会再出现high variance的情况。</p>
</li>
</ul>
<p>下面介绍两种使用weighted importance-sampling 方法实现的MC算法</p>
<h4 id="Oﬀ-policy-MC-prediction-policy-evaluation"><a href="#Oﬀ-policy-MC-prediction-policy-evaluation" class="headerlink" title="Oﬀ-policy MC prediction (policy evaluation)"></a>Oﬀ-policy MC prediction (policy evaluation)</h4><p>这种方法基于增量、加权取样、指数加权平均技术实现，算法主体见下图：</p>
<p><img src="5.4.png" alt=""></p>
<p>实际上当目标策略与行为策略高度重合时，也就是$W = 1$的时候，这个同时也是on-policy算法。</p>
<h4 id="Oﬀ-policy-Monte-Carlo-Control-value-iteration"><a href="#Oﬀ-policy-Monte-Carlo-Control-value-iteration" class="headerlink" title="Oﬀ-policy Monte Carlo Control(value iteration)"></a>Oﬀ-policy Monte Carlo Control(value iteration)</h4><p>考虑到目标策略与数据并不进行交互，因此可以随时对目标策略进行更新。并且规定目标策略为确定性策略，也就是$\pi(A_t|S_t) = 1$，因此可以注意到对$W$进行更新时，分子为1。</p>
<p><img src="5.5.png" alt=""></p>

      
    </div>
    
    
    

    

    

    
      <div>
        <ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>Post author:</strong>
    秦baibai
  </li>
  <li class="post-copyright-link">
    <strong>Post link:</strong>
    <a href="http://notes-hongbo.top/2018/12/19/Reinforcement-Learning-Note-2/" title="Reinforcement Learning Note (2)">http://notes-hongbo.top/2018/12/19/Reinforcement-Learning-Note-2/</a>
  </li>
  <li class="post-copyright-license">
    <strong>Copyright Notice: </strong>
    All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/3.0/" rel="external nofollow" target="_blank">CC BY-NC-SA 3.0</a> unless stating additionally.
  </li>
</ul>

      </div>
    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/Reinforcement-Learning/" rel="tag"># Reinforcement Learning</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2018/12/08/A-Simple-Interpreter/" rel="next" title="A Simple Interpreter">
                <i class="fa fa-chevron-left"></i> A Simple Interpreter
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2019/02/02/My-first-RL-related-paper/" rel="prev" title="My first RL-related paper">
                My first RL-related paper <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
      <div id="sidebar-dimmer"></div>
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope="" itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image" src="/images/avatar.JPG" alt="秦baibai">
            
              <p class="site-author-name" itemprop="name">秦baibai</p>
              <p class="site-description motion-element" itemprop="description">So we beat on, boats against the current, borne back ceaselessly into the past.</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives">
              
                  <span class="site-state-item-count">39</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">4</span>
                  <span class="site-state-item-name">categories</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">42</span>
                  <span class="site-state-item-name">tags</span>
                </a>
              </div>
            

          </nav>

          
            <div class="feed-link motion-element">
              <a href="/atom.xml" rel="alternate">
                <i class="fa fa-rss"></i>
                RSS
              </a>
            </div>
          

          <div class="links-of-author motion-element">
            
              
                <span class="links-of-author-item">
                  <a href="https://github.com/qhb1001" target="_blank" title="GitHub">
                    
                      <i class="fa fa-fw fa-github"></i>GitHub</a>
                </span>
              
                <span class="links-of-author-item">
                  <a href="mailto:hongbo.qin.1001@gmail.com" target="_blank" title="E-Mail">
                    
                      <i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                </span>
              
                <span class="links-of-author-item">
                  <a href="https://www.instagram.com/qhb_1001/" target="_blank" title="Instagram">
                    
                      <i class="fa fa-fw fa-instagram"></i>Instagram</a>
                </span>
              
            
          </div>

          
          

          
          
            <div class="links-of-blogroll motion-element links-of-blogroll-inline">
              <div class="links-of-blogroll-title">
                <i class="fa  fa-fw fa-link"></i>
                友情链接
              </div>
              <ul class="links-of-blogroll-list">
                
                  <li class="links-of-blogroll-item">
                    <a href="http://mrsempress.top" title="Little Sun" target="_blank">Little Sun</a>
                  </li>
                
                  <li class="links-of-blogroll-item">
                    <a href="https://tobiaslee.top/" title="TobiasLee" target="_blank">TobiasLee</a>
                  </li>
                
                  <li class="links-of-blogroll-item">
                    <a href="https://aidenfan.github.io/" title="UdefinedF" target="_blank">UdefinedF</a>
                  </li>
                
                  <li class="links-of-blogroll-item">
                    <a href="http://blog.three7.cc/" title="37as" target="_blank">37as</a>
                  </li>
                
              </ul>
            </div>
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Chapter-4-Dynamic-Programming"><span class="nav-number">1.</span> <span class="nav-text">Chapter 4 Dynamic Programming</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Policy-Evaluation"><span class="nav-number">1.1.</span> <span class="nav-text">Policy Evaluation</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Policy-Improvement"><span class="nav-number">1.2.</span> <span class="nav-text">Policy Improvement</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Policy-Iteration"><span class="nav-number">1.3.</span> <span class="nav-text">Policy Iteration</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Value-Iteration"><span class="nav-number">1.4.</span> <span class="nav-text">Value Iteration</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Chapter-5-Monte-Carlo-Methods"><span class="nav-number">2.</span> <span class="nav-text">Chapter 5 Monte Carlo Methods</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Monte-Carlo-Prediction"><span class="nav-number">2.1.</span> <span class="nav-text">Monte Carlo Prediction</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#MC-Estimating-for-Action-Values"><span class="nav-number">2.2.</span> <span class="nav-text">MC Estimating for Action Values</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#MC-Exploring-Start"><span class="nav-number">2.2.1.</span> <span class="nav-text">MC Exploring Start</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#On-policy-via-epsilon-soft"><span class="nav-number">2.2.2.</span> <span class="nav-text">On-policy via $\epsilon$-soft</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Oﬀ-policy-Prediction-via-Importance-Sampling"><span class="nav-number">2.2.3.</span> <span class="nav-text">Oﬀ-policy Prediction via Importance Sampling</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Oﬀ-policy-MC-prediction-policy-evaluation"><span class="nav-number">2.2.3.1.</span> <span class="nav-text">Oﬀ-policy MC prediction (policy evaluation)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Oﬀ-policy-Monte-Carlo-Control-value-iteration"><span class="nav-number">2.2.3.2.</span> <span class="nav-text">Oﬀ-policy Monte Carlo Control(value iteration)</span></a></li></ol></li></ol></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">秦baibai</span>

  
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-area-chart"></i>
    </span>
    
      <span class="post-meta-item-text">Site words total count&#58;</span>
    
    <span title="Site words total count">66.8k</span>
  
</div>






  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Muse</a></div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
          <span id="scrollpercent"><span>0</span>%</span>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.3"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.3"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.3"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.3"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.3"></script>




  


  




	





  





  












  





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  
  <script type="text/javascript" src="/js/src/js.cookie.js?v=5.1.3"></script>
  <script type="text/javascript" src="/js/src/scroll-cookie.js?v=5.1.3"></script>


  


</body>
</html>
