<p><em>Reinforcement Learning: An Introduction</em> 学习笔记（2）</p>
<p>[TOC]</p>
<h1 id="Chapter-4-Dynamic-Programming"><a href="#Chapter-4-Dynamic-Programming" class="headerlink" title="Chapter 4 Dynamic Programming"></a>Chapter 4 Dynamic Programming</h1><h2 id="Policy-Evaluation"><a href="#Policy-Evaluation" class="headerlink" title="Policy Evaluation"></a>Policy Evaluation</h2><p>基于策略的迭代算法，实际上就是穷举遍历的过程。在P93的图中，我可以预见，在第十六步之后，整个图的值函数将不会再发生巨大的变化，而是趋于一个稳定的状态。</p>
<p>这里需要注意的是，我们计算的对象，是一个确定策略的情景。也只有在确定策略的情景下，我们才可以明确出value function。</p>
<p><img src="4.1.png" alt=""></p>
<h2 id="Policy-Improvement"><a href="#Policy-Improvement" class="headerlink" title="Policy Improvement"></a>Policy Improvement</h2><p>$$<br>v_{\pi’}(s) \ge v_{\pi}(s)<br>$$</p>
<p>上式描述了，当新策略的每一步要么与原策略的收益相同，要么优于原策略，那么称当前的策略更优。于是我们可以使用贪心策略来启发式地从一个基础策略出发，对原有策略不断进行更新。<br>$$<br>\begin{equation}<br>\begin{aligned}<br>\pi’(s) &amp;=  \mathop{\arg\max}<em>{a}q</em>\pi(s,a) \<br>&amp;= \mathop{\arg\max}<em>a\mathbb{E}\big[R</em>{t+1} + \gamma v_\pi(S_{t+1})\ | \ S_t=s, A_t=a \big]<br>\end{aligned}<br>\end{equation}<br>$$<br>如此一来可以保证，当前策略一定优于原有策略。只是，针对某些问题，局部最优解不一定引向全局最优解。</p>
<h2 id="Policy-Iteration"><a href="#Policy-Iteration" class="headerlink" title="Policy Iteration"></a>Policy Iteration</h2><p>基于以上两点讨论，我们可以得到一个改进策略的基本方法<br>$$<br>\pi_0 \overset{\text{E}}{\rightarrow} v_{\pi_0} \overset{\text{I}}{\rightarrow} \pi_1 \overset{\text{E}}{\rightarrow} v_{\pi_1} \overset{\text{I}}{\rightarrow} \pi_2 \overset{\text{E}}{\rightarrow} \dots \overset{\text{I}}{\rightarrow} \pi_<em> \overset{\text{E}}{\rightarrow} v_{</em>}<br>$$<br>这里E表示evaluation，I表示Improvement。通过贪心策略我们可以保证，新生成的策略一定比原有策略更优。</p>
<p><img src="4.2.png" alt=""></p>
<p>由于我们使用的MDP是有限状态的，这也就表示状态的个数是有限的，因此在有限迭代之后一定可以找到一个最优值。不过该最优值可能是局部最优解。更加形式化的证明可以参见<a href="https://stats.stackexchange.com/questions/97083/why-does-policy-iteration-algorithm-converge-to-optimal-value-reinforcement-le">这里</a>。</p>
<h2 id="Value-Iteration"><a href="#Value-Iteration" class="headerlink" title="Value Iteration"></a>Value Iteration</h2><p><img src="4.4.png" alt=""></p>
<p>上图展示了value iteration的工作方式。类比于上面论述的方法，这里的特点是，只有一次policy improvement，因此收敛得更快。</p>
<p><img src="4.3.png" alt=""></p>
<p>上图展示了两者的区别。很明显，value iteration丢失了某些信息，以达到更快的迭代速度。</p>
<p>类比到梯度下降法中的话，可以认为policy是batch gradient descent，而后者这是stochastic gradient descent。</p>
<p><a href="https://medium.com/@jaems33/gamblers-problem-b4e91040e58a">这篇博客</a>很好地介绍了value iteration的工作方式，并且对gamble’s problem的最终结果进行了思考，得到了一个较好的解释：教材中的结果是不可复现的，具体的策略依赖于最终目标的数学特征。如1024由于是一个2的幂次，所以将会出现在2的幂次上面赌徒应该投入更多的筹码。</p>
<h1 id="Chapter-5-Monte-Carlo-Methods"><a href="#Chapter-5-Monte-Carlo-Methods" class="headerlink" title="Chapter 5 Monte Carlo Methods"></a>Chapter 5 Monte Carlo Methods</h1><p>蒙特卡洛方法，是一种可以通过学习来不断估计值函数，并且发现最优策略的方法。不像DP方法需要知道环境中的全部信息，蒙特卡洛方法只需要一些经验(experience) — 也就是通过与环境进行交互得到的必要信息。从这些信息中进行学习以达到最优行为。尽管蒙特卡洛方法也是需要建模的，但是这个模型只需要简单的转移即可，并不需要像DP那样的每一个状态的概率分布都必须建立。</p>
<p>这里只讨论处理episodic tasks的蒙特卡洛方法。</p>
<p>MC被广泛地应用于含有随机的情景下。MC与多臂赌博机问题有很多相似之处 – 都是通过采样、取均值的方法对state-action二元组进行估计；主要的不同是这里存在着多种状态，每一个状态都单独是一个多臂赌博机问题，并且各个状态之间是相互关联的。也就是说，在同一阶段，当前状态采取的行为将会影响下一个状态采取的行为获得的收益。但是相对于DP方法，各个状态之间又是相互独立的，原因是在DP中估计值函数需要一个状态之后的所有状态，而蒙特卡洛方法只需要一个状态而已，也就是采样到的那一个收益。</p>
<p>对于值函数与策略的迭代，MC同样使用了GPI(Generalized Policy Iteration)。</p>
<p>相对于DP，MC方法的优点总结如下：</p>
<ul>
<li>能够直接从经验中学习，而不需要建立理论模型</li>
<li>能够避免$p(s’, r|s, a)$的计算</li>
<li>能够只关注重要的状态而不去关心其它次要状态</li>
</ul>
<h2 id="Monte-Carlo-Prediction"><a href="#Monte-Carlo-Prediction" class="headerlink" title="Monte Carlo Prediction"></a>Monte Carlo Prediction</h2><p>在多臂赌博机问题中，预测state-action函数时，最简单的方法就是累积求和取平均，如此一来在试验次数足够大的情况下就可以收敛到真值。这是MC方法的核心思维。</p>
<p>在模型已知的情况下，通过收集每一个episode中经过某个状态的信息，我们就可以估计出该状态的值函数。估计方法有两种，一种称为<em>first-visit MC method</em>，表示只使用每一个episode中第一次访问该状态得到的信息；另一种方法称为<em>every-visit MC method</em>，表示使用每一次访问该状态得到的信息。这里只讨论第一种方法。</p>
<p><img src="5.1.png" alt=""></p>
<p>这个算法流程中有两点需要注意的地方：</p>
<ul>
<li>对于收益的计算是逆序进行的，其实是一种类似前缀和的思想。我们关心的所谓从一个状态出发的收益，正是<code>pre[n] - pre[i]</code>，这就是逆序求和的原因所在。</li>
<li><code>unless</code>在这里判断的，是要看该状态在本阶段是否已经计算过。因此本算法是<em>first-visit MC method</em>。</li>
</ul>
<h2 id="MC-Estimating-for-Action-Values"><a href="#MC-Estimating-for-Action-Values" class="headerlink" title="MC Estimating for Action Values"></a>MC Estimating for Action Values</h2><p> 当不存在模型的时候，就没有办法定义状态带来的收益了。因此想要估计的函数发生了改变，此时我们关心的是行为收益；当每一个状态后续的各种行为收益被估算出来之后，就可以很容易地进行策略更新。</p>
<p>此时我们估计的值就变成了state-action。同样的，估计方式有两个，分别是<em>first-visit MC method</em>以及<em>every-visit MC method</em>。含义与上述的基本一致。</p>
<p>这样的算法可能出现一些问题：</p>
<ul>
<li>在确定性策略的情况下，有些状态行为也许永远也访问不到。</li>
<li>我们使用的数据不可能包含有无限组数据，因此最终的结果一定是渐进收敛到真值，但不是真值。</li>
</ul>
<p>第一个问题实际上就是多臂赌博机问题中遇到的EE问题；而第二个问题在DP中也遇到了，这个问题存在于所有算法之中。</p>
<h3 id="MC-Exploring-Start"><a href="#MC-Exploring-Start" class="headerlink" title="MC Exploring Start"></a>MC Exploring Start</h3><p>初始化时，随机一点，允许探索行为。</p>
<p><img src="5.2.png" alt=""></p>
<p>缺点是不切实际，相当麻烦。</p>
<h3 id="On-policy-via-epsilon-soft"><a href="#On-policy-via-epsilon-soft" class="headerlink" title="On-policy via $\epsilon$-soft"></a>On-policy via $\epsilon$-soft</h3><p>MC方法中有两类方法可以良好地解决第一点问题，分别是on-policy, off-policy。这里首先介绍前者。</p>
<p>On-policy方法，一般来说都具有soft的特性，也就是$\pi(a|s)&gt;0$。但是在策略优化步骤中将会逐渐得演变为确定性策略。下面给出算法实现：</p>
<p><img src="5.3.png" alt=""></p>
<p>对于非贪心策略的可能性赋值那里很自然：每一个都有平等的机会被选中。但是对于贪心策略下的行为，这里将其被选中的策略规定为$1 - \epsilon + \epsilon/|A(S_t)|$。这是把$\epsilon$-soft思想放入算法内部的实现方式，具体的原因见下面的推导：<br>$$<br>\begin{equation}<br>\begin{aligned}<br>q_pi(s, \pi’(s)) &amp;= \sum_a \pi’(a|s)q_\pi(s,a) \<br>&amp;=\frac{\epsilon}{|A(s)|}\sum_aq_\pi(s,a) + (1-\epsilon)\max_aq_\pi(s,a) \<br>&amp;\ge \frac{\epsilon}{|A(s)|}\sum_aq_\pi(s,a)+(1-\epsilon)\sum_a\frac{\pi(a|s) - \frac{\epsilon}{|A(s)|}}{1-\epsilon}q_\pi(s,a) \<br>&amp;= \frac{\epsilon}{|A(s)|}\sum_aq_\pi(s,a)-\frac{\epsilon}{|A(s)|}\sum_aq_\pi(s,a) + \sum_a\pi(a|s)q_\pi(s,a) \<br>&amp;= v_\pi(s)<br>\end{aligned}<br>\end{equation}<br>$$<br>当然还有一种很自然的实现方式，那就是将探索的分类放到$\pi$函数外面。首先选取一个随机数，与$\epsilon$进行比较，大于该随机数时选择贪心策略，否则从非贪心策略中随机选择一个行为。这两种实现方法是等价的。</p>
<h3 id="Oﬀ-policy-Prediction-via-Importance-Sampling"><a href="#Oﬀ-policy-Prediction-via-Importance-Sampling" class="headerlink" title="Oﬀ-policy Prediction via Importance Sampling"></a>Oﬀ-policy Prediction via Importance Sampling</h3><p>在上面讨论的算法中，可以发现这里面存在着一个矛盾：一方面想要根据值函数来确定出策略，一方面想要进行探索以发现更好的值函数。因此On-Policy最终的策略包含了一定的探索成分，这使得其不是最优策略。一个显而易见的解决方法是维护两个策略，一个表示最终的目标策略(target policy)，一个用于产生行为的行为策略(behavior policy)。在这里，发生了数据与最终策略的分离，因此称为<em>off-policy learning</em>。</p>
<p>具体的做法是采用所谓的<em>importance-sampling</em>，也就是利用从另一个分布采样得到的值，来估计这一个分布中的期望值。给定起始状态$S_t$，以及后续状态直到$S_T$，在策略$\pi$下产生这一串决策的概率是：<br>$$<br>\begin{equation}<br>\begin{aligned}<br>\text{Pr}{&amp;A_t, S_{t+1},,A_{t+1},\dots,S_T|S_t, A_{t:T-1~\sim \pi} }\<br>&amp;=\pi(A_t|S_t)p(S_{t+1}|S_t, A_t)\pi(A_{t+1}|S_{t+1})\cdots p(S_T|S_{T-1},A_{T-1}) \<br>&amp;= \prod_{k=t}^{T-1}\pi(A_k|S_k)p(S_{k+1}|S_k,A_k)<br>\end{aligned}<br>\end{equation}<br>$$<br>在上式的基础上，产生<em>importance-sampling ratio</em>：<br>$$<br>\rho_{t:T-1} = \frac{\prod_{k=t}^{T-1}\pi(A_k|S_k)p(S_{k+1}|S_k,A_k)}{\prod_{k=t}^{T-1}b(A_k|S_k)p(S_{k+1}|S_k,A_k)} = \prod_{k=t}^{T-1}\frac{\pi(A_k|S_k)}{b(A_k|S_k)}<br>$$<br>可以观察到这是一个与模型无关，只与策略相关的方法。此时我们就可以通过策略$b$的观测值来得到策略$\pi$的期望值：<br>$$<br>\mathbb{E}[\rho_{t:T-1}G_t|S_t] = v_\pi(S_t)<br>$$<br>在具体实现的时候，我们有两种方式可以选择：</p>
<ul>
<li><p>ordinary importance sampling</p>
<p>这种方式对目标策略的估计公式为$V(s) = \displaystyle \frac{\sum_{t\in\mathcal{J}(s)}\rho_{t:T(t)-1}G_t}{|\mathcal{J}(s)|}$</p>
<p>表示对于每一次采样结果都平等对待。考虑当只有一次采样结果并且与实际值偏差较大的情况，那么可能会出现偏差非常大的结果。因此这是high variance的算法。</p>
</li>
<li><p>weighted importance sampling</p>
<p>这种方式对目标策略的估计公式为$V(s) = \displaystyle \frac{\sum_{t\in\mathcal{J}(s)}\rho_{t:T(t)-1}G_t}{\sum_{t\in\mathcal{J}(s)}\rho_{t:T(t)-1}} $</p>
<p>如此一来，每一个采样结果将会根据importance-sampling ratio的值进行区别对待。因此不会再出现high variance的情况。</p>
</li>
</ul>
<p>下面介绍两种使用weighted importance-sampling 方法实现的MC算法</p>
<h4 id="Oﬀ-policy-MC-prediction-policy-evaluation"><a href="#Oﬀ-policy-MC-prediction-policy-evaluation" class="headerlink" title="Oﬀ-policy MC prediction (policy evaluation)"></a>Oﬀ-policy MC prediction (policy evaluation)</h4><p>这种方法基于增量、加权取样、指数加权平均技术实现，算法主体见下图：</p>
<p><img src="5.4.png" alt=""></p>
<p>实际上当目标策略与行为策略高度重合时，也就是$W = 1$的时候，这个同时也是on-policy算法。</p>
<h4 id="Oﬀ-policy-Monte-Carlo-Control-value-iteration"><a href="#Oﬀ-policy-Monte-Carlo-Control-value-iteration" class="headerlink" title="Oﬀ-policy Monte Carlo Control(value iteration)"></a>Oﬀ-policy Monte Carlo Control(value iteration)</h4><p>考虑到目标策略与数据并不进行交互，因此可以随时对目标策略进行更新。并且规定目标策略为确定性策略，也就是$\pi(A_t|S_t) = 1$，因此可以注意到对$W$进行更新时，分子为1。</p>
<p><img src="5.5.png" alt=""></p>
